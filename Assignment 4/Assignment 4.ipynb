{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "S8PKiVJaL_AW",
    "outputId": "23c2a4cd-0c53-4a1c-c454-7ed87e392a8c"
   },
   "outputs": [],
   "source": [
    "# Imports and pip installations (if needed)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A05Q5B0_NUX9"
   },
   "source": [
    "# Part 1: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "YZ4MUsbXNZ0P",
    "outputId": "77e7a628-792f-4d28-b7b1-e06f4db3efd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                 5.1               3.5                1.4               0.2   \n",
      "1                 4.9               3.0                1.4               0.2   \n",
      "2                 4.7               3.2                1.3               0.2   \n",
      "3                 4.6               3.1                1.5               0.2   \n",
      "4                 5.0               3.6                1.4               0.2   \n",
      "5                 5.4               3.9                1.7               0.4   \n",
      "6                 4.6               3.4                1.4               0.3   \n",
      "7                 5.0               3.4                1.5               0.2   \n",
      "8                 4.4               2.9                1.4               0.2   \n",
      "9                 4.9               3.1                1.5               0.1   \n",
      "10                5.4               3.7                1.5               0.2   \n",
      "11                4.8               3.4                1.6               0.2   \n",
      "12                4.8               3.0                1.4               0.1   \n",
      "13                4.3               3.0                1.1               0.1   \n",
      "14                5.8               4.0                1.2               0.2   \n",
      "\n",
      "    target  \n",
      "0      0.0  \n",
      "1      0.0  \n",
      "2      0.0  \n",
      "3      0.0  \n",
      "4      0.0  \n",
      "5      0.0  \n",
      "6      0.0  \n",
      "7      0.0  \n",
      "8      0.0  \n",
      "9      0.0  \n",
      "10     0.0  \n",
      "11     0.0  \n",
      "12     0.0  \n",
      "13     0.0  \n",
      "14     0.0  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset (load remotely, not locally)\n",
    "irisData = load_iris()\n",
    "# Output the first 15 rows of the data\n",
    "# Display a summary of the table information (number of datapoints, etc.)\n",
    "df = pd.DataFrame(data= np.c_[irisData['data'], irisData['target']], columns= irisData['feature_names'] + ['target'])\n",
    "print(df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sepal length (cm)  150 non-null    float64\n",
      " 1   sepal width (cm)   150 non-null    float64\n",
      " 2   petal length (cm)  150 non-null    float64\n",
      " 3   petal width (cm)   150 non-null    float64\n",
      " 4   target             150 non-null    float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 6.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRMtsJKBaxWu"
   },
   "source": [
    "## About the dataset\n",
    "Explain what the data is in your own words. What are your features and labels? What is the mapping of your labels to the actual classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 150 rows with no null values, there are total of 5 columns namely sepal length (in cm), sepal width (in cm), petal length (in cm), petal width (in cm), and target (0 for Iris setosa, 1 for Iris versicolor, and 2 for Iris virginica)<br>\n",
    "\n",
    "The features for this model are:\n",
    "- sepal length (cm)\n",
    "- sepal width (cm)\n",
    "- petal length (cm)\n",
    "- petal width (cm) \n",
    "\n",
    "and the label is:\n",
    "\n",
    "- target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhKaIrZKNaHg"
   },
   "source": [
    "# Part 2: Split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrgogB62NcOi",
    "outputId": "7bbfb1ea-88ed-4944-ab94-48696b9a71df"
   },
   "outputs": [],
   "source": [
    "# Take the dataset and split it into our features (X) and label (y)\n",
    "X = irisData.data\n",
    "y = irisData.target\n",
    "# Use sklearn to split the features and labels into a training/test set. (90% train, 10% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.90, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hblF-ei9Ncia"
   },
   "source": [
    "# Part 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IhFhEN03Nf7o",
    "outputId": "15a94661-020b-4520-b5f3-c34fdfb042f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class = setosa and confidence = 83.72%\n",
      "predicted class = versicolor and confidence = 94.95%\n",
      "predicted class = virginica and confidence = 99.89%\n",
      "Accuracy of Logistic Regression Classifier on test set: 1.0\n",
      "Confusion Matrix:\n",
      "[[6 0 0]\n",
      " [0 6 0]\n",
      " [0 0 3]]\n",
      "intercept: [  9.50134804   1.90990729 -11.41125533] \n",
      "coeff: [[-0.42689859  0.51325838 -0.08635979]\n",
      " [ 0.97268339 -0.22357238 -0.74911101]\n",
      " [-2.4446162  -0.21492369  2.6595399 ]\n",
      " [-1.03175179 -0.85147306  1.88322485]]\n"
     ]
    }
   ],
   "source": [
    "# i. Use sklearn to train a LogisticRegression model on the training set\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(X_train, y_train)\n",
    "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
    "y_prediction = logReg.predict(X_test)\n",
    "y_prediction_prob = logReg.predict_proba(X_test)\n",
    "classes = load_iris().target_names\n",
    "for class_name, proba in zip(classes, y_prediction_prob):\n",
    "    index = np.argmax(proba)\n",
    "    print(f'predicted class = {class_name} and confidence = {proba[index]:.2%}')\n",
    "# iii. Report on the score for Logistic regression model, what does the score measure?\n",
    "score = logReg.score(X_test, y_test)\n",
    "print('Accuracy of Logistic Regression Classifier on test set: {}'.format(score))\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_prediction)\n",
    "print(f'Confusion Matrix:\\n{confusion}')\n",
    "# iv. Extract the coefficents and intercepts for the boundary line(s)\n",
    "intercept = logReg.intercept_\n",
    "coeff = logReg.coef_.T\n",
    "print(f'intercept: {intercept} \\ncoeff: {coeff}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score of 100% shows that on the given 10% of the data, i.e. test set, our logistic regression classifier model predicts the outcome of 100%. <br>\n",
    "Also looking at the confusion matrix we can see that:<br>\n",
    "- for true positive of setosa we got 6 out of 6\n",
    "- for true positive of versicolor we got 6 out of 6\n",
    "- for true positive of virginica we got 3 out of 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDUpXQN4Nilk"
   },
   "source": [
    "# Part 4: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U__zukpdNqiQ",
    "outputId": "5cd81a50-9137-4f36-842e-0687e9bc7c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class = setosa and confidence = 90.79%\n",
      "predicted class = versicolor and confidence = 95.21%\n",
      "predicted class = virginica and confidence = 98.25%\n",
      "Accuracy of Support Vector Classifier on test set: 1.0\n",
      "Confusion Matrix:\n",
      "[[6 0 0]\n",
      " [0 6 0]\n",
      " [0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "# i. Use sklearn to train a Support Vector Classifier on the training set\n",
    "svc = svm.SVC(probability=True)\n",
    "svc.fit(X_train, y_train)\n",
    "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
    "y_prediction_svc = svc.predict(X_test)\n",
    "y_prediction_prob_svc = svc.predict_proba(X_test)\n",
    "for class_name, proba in zip(classes, y_prediction_prob_svc):\n",
    "    index = np.argmax(proba)\n",
    "    print(f'predicted class = {class_name} and confidence = {proba[index]:.2%}')\n",
    "# iii. Report on the score for the SVM, what does the score measure?\n",
    "score = svc.score(X_test, y_test)\n",
    "print('Accuracy of Support Vector Classifier on test set: {}'.format(score))\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_prediction_svc)\n",
    "print(f'Confusion Matrix:\\n{confusion}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as the Logistic Regression we also got the score of 100% for our Support Vector Machine model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULoL7mMBNrlS"
   },
   "source": [
    "# Part 5: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKKmODVrN9lQ",
    "outputId": "dced24f7-c6bd-45fb-c68f-87548e9228dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class = setosa and confidence = 91.73%\n",
      "predicted class = versicolor and confidence = 99.78%\n",
      "predicted class = virginica and confidence = 100.00%\n",
      "Accuracy of Neural Network on test set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# i. Use sklearn to train a Neural Network (MLP Classifier) on the training set\n",
    "mlp = MLPClassifier(solver='sgd', learning_rate_init=0.01, hidden_layer_sizes=10, max_iter=500)\n",
    "mlp.fit(X_train, y_train)\n",
    "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
    "y_prediction_mlp = mlp.predict(X_test)\n",
    "y_prediction_prob_mlp = mlp.predict_proba(X_test)\n",
    "for class_name, proba in zip(classes, y_prediction_prob_mlp):\n",
    "    index = np.argmax(proba)\n",
    "    print(f'predicted class = {class_name} and confidence = {proba[index]:.2%}')\n",
    "# iii. Report on the score for the Neural Network, what does the score measure?\n",
    "score = mlp.score(X_test, y_test)\n",
    "print('Accuracy of Neural Network on test set: {}'.format(score))\n",
    "# iv: Experiment with different options for the neural network, report on your best configuration (the highest score I was able to achieve was 0.8666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bi5tDwHmC28"
   },
   "source": [
    "# Part 6: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCFlfJy2mCg6",
    "outputId": "e71bf88c-6418-4b7f-e289-4acf743c16cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class = setosa and confidence = 100.00%\n",
      "predicted class = versicolor and confidence = 100.00%\n",
      "predicted class = virginica and confidence = 100.00%\n",
      "Accuracy of k-Neighbors on test set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# i. Use sklearn to 'train' a k-Neighbors Classifier\n",
    "# Note: KNN is a nonparametric model and technically doesn't require training\n",
    "# fit will essentially load the data into the model see link below for more information\n",
    "# https://stats.stackexchange.com/questions/349842/why-do-we-need-to-fit-a-k-nearest-neighbors-classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "knn.fit(X_train, y_train)\n",
    "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
    "y_prediction_knn = knn.predict(X_test)\n",
    "y_prediction_prob_knn = knn.predict_proba(X_test)\n",
    "for class_name, proba in zip(classes, y_prediction_prob_knn):\n",
    "    index = np.argmax(proba)\n",
    "    print(f'predicted class = {class_name} and confidence = {proba[index]:.2%}')\n",
    "# iii. Report on the score for kNN, what does the score measure?\n",
    "score = knn.score(X_test, y_test)\n",
    "print('Accuracy of k-Neighbors on test set: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "162Sw3LeoqE2"
   },
   "source": [
    "# Part 7: Conclusions and takeaways\n",
    "\n",
    "In your own words describe the results of the notebook. Which model(s) performed the best on the dataset? Why do you think that is? Did anything surprise you about the exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we use the Classification Model, as we were ask to classify; given some features like sepal length (cm), sepal width (cm), petal length (cm), petal width (cm); into 3 classes of the iris plant. <br>\n",
    "We use 4 different types of Classifier to solve this assignment and compare the result for each of those: \n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "- Neural Network\n",
    "- K-Nearest Neighbors\n",
    "\n",
    "Though all 4 model give us 100% accuracy, K Nearest Neighbors gave 100% probabilities for each class, it is because for a plant to be in that class it has to be within the range of its respective features, if its smaller or bigger than that range it belongs to the different class. <br>\n",
    "\n",
    "One thing that surprise me is that we getting 100% accuracy for all the models, I am not sure if its because of the dataset or because of the spliting of data into 90% test data and 10% data i.e. having less number of data to test with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
